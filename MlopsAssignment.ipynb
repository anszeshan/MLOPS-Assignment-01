{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IQ8IivtS-8Ss"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#part 1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, render_template, jsonify\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "#part 1\n",
        "import pandas as pd\n",
        "from flask import Flask, render_template, jsonify\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the main dataset\n",
        "main_dataset_url = \"malicious_phish.csv\"  # Replace with your file path or URL\n",
        "main_data = pd.read_csv(main_dataset_url)\n",
        "\n",
        "# Create a DataFrame for the helper data (second file with URLs)\n",
        "helper_data = pd.read_csv(\"spam_dataset.csv\", header=None, names=[\"url\"])\n",
        "helper_data[\"type\"] = \"spam\"  # Add a 'type' column with value 'spam'\n",
        "\n",
        "# Merge the datasets based on a common key or concatenate them\n",
        "merged_data = pd.concat([main_data, helper_data], ignore_index=True)\n",
        "\n",
        "# Display the merged dataset\n",
        "print(merged_data.head())\n",
        "\n",
        "# Save the merged dataset to a new CSV file\n",
        "merged_data.to_csv(\"merged_dataset_with_spam.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KWsdafCmAIzz"
      },
      "outputs": [],
      "source": [
        "#part 2\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the merged dataset\n",
        "merged_dataset = pd.read_csv(\"merged_dataset_with_spam.csv\")  # Replace with your file path\n",
        "\n",
        "# Handle missing values (if any)\n",
        "merged_dataset.dropna(inplace=True)  # For demonstration, dropping rows with any NaN values\n",
        "\n",
        "# Data cleaning: Removing duplicates\n",
        "merged_dataset.drop_duplicates(inplace=True)\n",
        "# Encoding categorical variables\n",
        "# Assuming 'type' column is categorical\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_types = pd.get_dummies(merged_dataset['type'], prefix='type')\n",
        "merged_dataset = pd.concat([merged_dataset, encoded_types], axis=1)\n",
        "merged_dataset.drop('type', axis=1, inplace=True)  # Drop the original 'type' column\n",
        "# Save the preprocessed dataset to a new CSV file\n",
        "merged_dataset.to_csv(\"preprocessed_merged_dataset.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ChmXtRWeALa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#part 3\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load your dataset\n",
        "dataset = pd.read_csv(\"preprocessed_merged_dataset.csv\")\n",
        "\n",
        "# Separate the data into benign and malicious classes\n",
        "benign_data = dataset[dataset['type_benign'] == True]\n",
        "malicious_data = dataset[dataset['type_benign'] == False]\n",
        "\n",
        "# Find the class with the highest count (usually benign) for reference\n",
        "max_class_count = benign_data.shape[0]\n",
        "\n",
        "# Resample each malicious class to match the count of the benign class\n",
        "resampled_data = []\n",
        "for column in ['type_defacement', 'type_malware', 'type_phishing', 'type_spam']:\n",
        "    malicious_class = malicious_data[malicious_data[column] == True]\n",
        "    resampled_class = resample(malicious_class, n_samples=max_class_count, random_state=42)\n",
        "    resampled_data.append(resampled_class)\n",
        "\n",
        "# Combine the resampled malicious classes with the original benign class\n",
        "balanced_dataset = pd.concat([benign_data] + resampled_data)\n",
        "\n",
        "# Shuffle the dataset to ensure randomness\n",
        "balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a new CSV file\n",
        "balanced_dataset.to_csv(\"balanced_dataset.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF5NqdiFAQJU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#part4\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your balanced dataset\n",
        "dataset = pd.read_csv(\"balanced_dataset.csv\")  # Replace with your dataset filename\n",
        "\n",
        "# Descriptive statistics\n",
        "print(\"Descriptive Statistics:\")\n",
        "print(dataset.describe())\n",
        "\n",
        "# Data distributions for each feature\n",
        "print(\"\\nData Distributions:\")\n",
        "for column in dataset.columns:\n",
        "    if dataset[column].dtype == 'bool':\n",
        "        # Plot counts for boolean columns\n",
        "        sns.countplot(x=column, data=dataset)\n",
        "    else:\n",
        "        # Plot histograms for numeric columns\n",
        "        sns.histplot(data=dataset, x=column, bins=20, kde=True)\n",
        "    plt.title(f\"Distribution of {column}\")\n",
        "    plt.show()\n",
        "\n",
        "# Explore relationships between features and target variable\n",
        "print(\"\\nRelationships with Target Variable (type_benign):\")\n",
        "for column in dataset.columns:\n",
        "    if column != 'type_benign':\n",
        "        if dataset[column].dtype == 'bool':\n",
        "            # Plot bar plots for boolean features\n",
        "            sns.barplot(x='type_benign', y=column, data=dataset, ci=None)\n",
        "        else:\n",
        "            # Plot box plots for numeric features\n",
        "            sns.boxplot(x='type_benign', y=column, data=dataset)\n",
        "        plt.title(f\"Relationship between {column} and type_benign\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-HLkYgYSJsB"
      },
      "outputs": [],
      "source": [
        "\n",
        "#part 5\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your balanced dataset\n",
        "dataset = pd.read_csv(\"balanced_dataset.csv\")  # Replace with your dataset filename\n",
        "\n",
        "# Plot 1: Distribution of Malicious Types\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=\"type_benign\", data=dataset)\n",
        "plt.title(\"Distribution of Malicious Types\")\n",
        "plt.xlabel(\"Malicious Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.savefig(\"plot1.png\")\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Correlation Heatmap\n",
        "correlation_matrix = dataset.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.savefig(\"plot2.png\")\n",
        "plt.show()\n",
        "\n",
        "# Extract URL length from the \"url\" column\n",
        "dataset['url_length'] = dataset['url'].str.len()\n",
        "\n",
        "# Plot 3: Box Plot of URL Length by Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=\"type_benign\", y=\"url_length\", data=dataset)\n",
        "plt.title(\"URL Length by Malicious Type\")\n",
        "plt.xlabel(\"Malicious Type\")\n",
        "plt.ylabel(\"URL Length\")\n",
        "plt.savefig(\"plot3.png\")\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Pairplot for Selected Features\n",
        "selected_features = [\"type_malware\", \"type_phishing\", \"type_spam\"]\n",
        "# Create a pairplot\n",
        "sns.pairplot(dataset[selected_features], hue=\"type_malware\")\n",
        "plt.suptitle(\"Pairplot of Selected Features by Malicious Type\")\n",
        "plt.savefig(\"plot4.png\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plot 5: Stacked Bar Plot of Categorical Features\n",
        "# Modify the list of categorical features based on your dataset columns\n",
        "categorical_features = [\"type_defacement\", \"type_malware\", \"type_phishing\", \"type_spam\"]\n",
        "\n",
        "# Create a new DataFrame for the categorical features\n",
        "categorical_data = dataset[categorical_features + [\"type_benign\"]]\n",
        "\n",
        "# Melt the DataFrame to prepare for the stacked bar plot\n",
        "stacked_data = categorical_data.melt(id_vars=\"type_benign\", var_name=\"Categorical Feature\")\n",
        "\n",
        "# Plot 5: Stacked Bar Plot of Categorical Features\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x=\"Categorical Feature\", hue=\"value\", data=stacked_data, hue_order=[True, False])\n",
        "plt.title(\"Stacked Bar Plot of Categorical Features by Malicious Type\")\n",
        "plt.xlabel(\"Categorical Feature\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig(\"plot5.png\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGf8Pla4SOHs"
      },
      "outputs": [],
      "source": [
        "\n",
        "#part 6\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your balanced dataset\n",
        "dataset = pd.read_csv(\"balanced_dataset.csv\")  # Replace with your dataset filename\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(url):\n",
        "    features = {}\n",
        "\n",
        "    # Feature 1: URL Length\n",
        "    features['url_length'] = len(url)\n",
        "\n",
        "    # Feature 2: Count of Special Characters\n",
        "    special_characters = re.compile(r\"[!@#$%^&*(),.?\\\":{}|<>]\")\n",
        "    features['special_char_count'] = len(re.findall(special_characters, url))\n",
        "\n",
        "    # Feature 3: Count of Digits\n",
        "    features['digit_count'] = len(re.findall(r\"\\d\", url))\n",
        "\n",
        "    # Feature 4: Count of Hyphens\n",
        "    features['hyphen_count'] = len(re.findall(r\"-\", url))\n",
        "\n",
        "    # Feature 5: Count of Dots\n",
        "    features['dot_count'] = len(re.findall(r\"\\.\", url))\n",
        "\n",
        "    return features\n",
        "\n",
        "# Apply feature extraction to the dataset\n",
        "dataset['url_features'] = dataset['url'].apply(extract_features)\n",
        "\n",
        "# Convert extracted features into separate columns\n",
        "feature_columns = pd.DataFrame(dataset['url_features'].tolist())\n",
        "dataset = pd.concat([dataset, feature_columns], axis=1)\n",
        "\n",
        "# Drop the original 'url_features' column\n",
        "dataset.drop('url_features', axis=1, inplace=True)\n",
        "\n",
        "# Save the dataset with extracted features to a new CSV file\n",
        "dataset.to_csv(\"dataset_with_extracted_features.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZYS02qNSQ4G"
      },
      "outputs": [],
      "source": [
        "\n",
        "#part 7\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load your balanced dataset\n",
        "dataset = pd.read_csv(\"dataset_with_extracted_features.csv\")  # Replace with your dataset filename\n",
        "\n",
        "# Define a list of columns to exclude from standardization\n",
        "columns_to_exclude = [\"url\"]  # Add any other non-numeric or non-boolean columns\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = dataset.drop(['type_benign'], axis=1) if 'type_benign' in dataset.columns else dataset  # Check if 'type_benign' exists\n",
        "\n",
        "# Remove columns to exclude from X\n",
        "X = X.drop(columns_to_exclude, axis=1)\n",
        "y = dataset['type_benign']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "y_pred_logistic = logistic_model.predict(X_test)\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_logistic)\n",
        "\n",
        "# Model 2: Random Forest\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-tFi_raST-n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#part 8\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load your dataset with extracted features\n",
        "dataset = pd.read_csv(\"dataset_with_extracted_features.csv\")  # Replace with your dataset filename\n",
        "\n",
        "# Exclude non-numeric columns (e.g., 'url') from scaling\n",
        "numeric_columns = dataset.select_dtypes(include=['number']).columns\n",
        "X = dataset[numeric_columns]\n",
        "y = dataset['type_benign']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "logistic_model = LogisticRegression(random_state=42)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Model 2: Random Forest\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.colorbar()\n",
        "    classes = ['Malicious', 'Benign']\n",
        "    tick_marks = range(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize Confusion Matrices\n",
        "plot_confusion_matrix(logistic_model, X_test, y_test)\n",
        "plot_confusion_matrix(random_forest_model, X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "app = Flask(__name__)\n",
        "@app.route('/')\n",
        "def visualize_data():\n",
        "    # Load your balanced dataset\n",
        "    dataset = pd.read_csv(\"balanced_dataset.csv\")\n",
        "\n",
        "    # Plot 1: Distribution of Malicious Types\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=\"type_benign\", data=dataset)\n",
        "    plt.title(\"Distribution of Malicious Types\")\n",
        "    plt.xlabel(\"Malicious Type\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.savefig(\"plot1.png\")\n",
        "\n",
        "    # Plot 2: Correlation Heatmap\n",
        "    correlation_matrix = dataset.corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.savefig(\"plot2.png\")\n",
        "\n",
        "    # Extract URL length from the \"url\" column\n",
        "    dataset['url_length'] = dataset['url'].str.len()\n",
        "\n",
        "    # Plot 3: Box Plot of URL Length by Type\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=\"type_benign\", y=\"url_length\", data=dataset)\n",
        "    plt.title(\"URL Length by Malicious Type\")\n",
        "    plt.xlabel(\"Malicious Type\")\n",
        "    plt.ylabel(\"URL Length\")\n",
        "    plt.savefig(\"plot3.png\")\n",
        "\n",
        "    # Plot 4: Pairplot for Selected Features\n",
        "    selected_features = [\"type_malware\", \"type_phishing\", \"type_spam\"]\n",
        "    sns.pairplot(dataset[selected_features], hue=\"type_malware\")\n",
        "    plt.suptitle(\"Pairplot of Selected Features by Malicious Type\")\n",
        "    plt.savefig(\"plot4.png\")\n",
        "\n",
        "    # Plot 5: Stacked Bar Plot of Categorical Features\n",
        "    categorical_features = [\"type_defacement\", \"type_malware\", \"type_phishing\", \"type_spam\"]\n",
        "    categorical_data = dataset[categorical_features + [\"type_benign\"]]\n",
        "    stacked_data = categorical_data.melt(id_vars=\"type_benign\", var_name=\"Categorical Feature\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.countplot(x=\"Categorical Feature\", hue=\"value\", data=stacked_data, hue_order=[True, False])\n",
        "    plt.title(\"Stacked Bar Plot of Categorical Features by Malicious Type\")\n",
        "    plt.xlabel(\"Categorical Feature\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.savefig(\"plot5.png\")\n",
        "\n",
        "    # Return the HTML template with the rendered graphs\n",
        "    return render_template('index.html')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
